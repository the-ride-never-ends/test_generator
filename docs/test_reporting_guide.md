# Test Reporting System Guide

The Test Generator Mk2 includes a comprehensive test reporting system that automatically generates detailed reports in both JSON and Markdown formats. This guide explains how to use and interpret these reports.

## Example Report

Here's an example of a Markdown test report generated by the system:

```markdown
# Test Generator Mk2 - Test Report
Generated on: 2025-04-13 21:13:47

## Summary

- **Tests Run**: 17
- **Passed**: 12
- **Failures**: 1
- **Errors**: 4
- **Skipped**: 0
- **Expected Failures**: 0
- **Unexpected Successes**: 0
- **Success Rate**: 70.59%
- **Duration**: 0.01 seconds

## Test Details

| Status | Module | Class | Test |
|--------|--------|-------|------|
| FAIL | test_utils | TestConvertToPascalCase | test_convert_already_pascal_case |
| ERROR | unittest.loader | _FailedTest | test_cli |
...
```

## Report Structure

### Summary Section

The summary section provides key metrics about the test run:

- **Tests Run**: Total number of tests executed
- **Passed**: Number of tests that passed
- **Failures**: Number of tests that failed with assertion errors
- **Errors**: Number of tests that raised unexpected exceptions
- **Skipped**: Number of tests that were skipped
- **Expected Failures**: Number of tests that were expected to fail
- **Unexpected Successes**: Number of tests that passed unexpectedly
- **Success Rate**: Percentage of tests that passed
- **Duration**: Total time taken for the test run

### Test Details Section

The test details section lists all tests that didn't pass, with:

- **Status**: FAIL, ERROR, SKIPPED, etc.
- **Module**: The module containing the test
- **Class**: The test class
- **Test**: The specific test method

### Failure and Error Details

For each failure or error, the report includes:

- **Message**: The error message or assertion failure
- **Traceback**: The complete stack trace for debugging

## Report Formats

The system generates reports in two formats:

1. **JSON**: Machine-readable format for programmatic analysis
2. **Markdown**: Human-readable format for quick review

## Using the Reports

### Finding Failed Tests

Look for tests with status "FAIL" in the test details section. These tests ran but had assertion failures, indicating that the code didn't behave as expected.

### Fixing Import Errors

Many errors in the example report are import errors:

```
ImportError: attempted relative import with no known parent package
```

This usually indicates an issue with the Python module structure or how the tests are being run. To fix:

1. Ensure the package is properly installed or in the Python path
2. Use absolute imports instead of relative imports
3. Run tests from the correct directory

### Fixing Missing Dependencies

Some errors indicate missing dependencies:

```
ModuleNotFoundError: No module named 'jinja2'
```

To fix, install the required dependency:

```bash
pip install jinja2
```

### Fixing Assertion Failures

Assertion failures show exactly what was expected versus what was received:

```
AssertionError: 'Helloworld' != 'HelloWorld'
- Helloworld
?      ^
+ HelloWorld
?      ^
```

This specific example shows a case sensitivity issue in the `convert_to_pascal_case` function.

## Generating Reports

Reports are automatically generated when you run tests:

```bash
# Run tests and generate reports
./run_tests.sh
```

Reports are stored in the `test_reports/` directory:

- `latest_report.json` - Always contains the most recent report (JSON)
- `latest_report.md` - Always contains the most recent report (Markdown)
- `test_report_TIMESTAMP.json` - Historical reports with timestamps (JSON)
- `test_report_TIMESTAMP.md` - Historical reports with timestamps (Markdown)

## Viewing Reports

Use the view_report.py script to view reports with different detail levels:

```bash
# View latest report summary
python view_report.py

# View with more details
python view_report.py --format details

# View complete details
python view_report.py --format full

# View a specific report
python view_report.py test_reports/test_report_20250413_211347.md
```

## Continuous Integration

The test reporting system can be integrated into CI/CD pipelines:

1. Run tests as part of the pipeline
2. Analyze the JSON report to determine pass/fail status
3. Store reports as build artifacts
4. Track success rates over time